{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nyykol/Battery-Indicator/blob/main/Train_nnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YOLOv5 Classification Tutorial\n",
        "\n",
        "YOLOv5 supports classification tasks too. This is the official YOLOv5 classification notebook tutorial. YOLOv5 is maintained by [Ultralytics](https://github.com/ultralytics/yolov5).\n",
        "\n",
        "This notebook covers:\n",
        "\n",
        "*   Inference with out-of-the-box YOLOv5 classification on ImageNet\n",
        "*  [Training YOLOv5 classification](https://blog.roboflow.com//train-YOLOv5-classification-custom-data) on custom data\n",
        "\n",
        "*Looking for custom data? Explore over 66M community datasets on [Roboflow Universe](https://universe.roboflow.com).*\n",
        "\n",
        "This notebook was created with Google Colab. [Click here](https://colab.research.google.com/drive/1FiSNz9f_nT8aFtDEU3iDAQKlPT8SCVni?usp=sharing) to run it."
      ],
      "metadata": {
        "id": "5GYQX3of4QiW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "Pull in respective libraries to prepare the notebook environment."
      ],
      "metadata": {
        "id": "-PJ8vlYXbWtN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pIM7fOwm8A7l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5344acb4-aff4-48d8-c4d0-7492d3134dc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 üöÄ v7.0-210-gdd10481 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ‚úÖ (2 CPUs, 12.7 GB RAM, 26.3/78.2 GB disk)\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt  # install\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()  # checks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Infer on ImageNet\n",
        "\n",
        "To demonstrate YOLOv5 classification, we'll leverage an already trained model. In this case, we'll download the ImageNet trained models pretrained on ImageNet using YOLOv5 Utils."
      ],
      "metadata": {
        "id": "i_DrUi2nmF40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.downloads import attempt_download\n",
        "\n",
        "p5 = ['n', 's', 'm', 'l', 'x']  # P5 models\n",
        "cls = [f'{x}-cls' for x in p5]  # classification models\n",
        "\n",
        "for x in cls:\n",
        "    attempt_download(f'weights/yolov5{x}.pt')"
      ],
      "metadata": {
        "id": "o2scLEh6EYnL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee1424f1-f3cd-42ed-eabd-a5ffdb80c2a3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n-cls.pt to weights/yolov5n-cls.pt...\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.87M/4.87M [00:00<00:00, 62.3MB/s]\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s-cls.pt to weights/yolov5s-cls.pt...\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10.5M/10.5M [00:00<00:00, 18.6MB/s]\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m-cls.pt to weights/yolov5m-cls.pt...\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24.9M/24.9M [00:00<00:00, 60.8MB/s]\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l-cls.pt to weights/yolov5l-cls.pt...\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50.9M/50.9M [00:01<00:00, 49.1MB/s]\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x-cls.pt to weights/yolov5x-cls.pt...\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 92.0M/92.0M [00:02<00:00, 44.7MB/s]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can infer on an example image from the ImageNet dataset."
      ],
      "metadata": {
        "id": "Fn2_a38DmZ2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #Download example image\n",
        "# import requests\n",
        "# image_url = \"https://i.imgur.com/OczPfaz.jpg\"\n",
        "# # img_data = requests.get(image_url).content\n",
        "# with open('bananas.jpg', 'wb') as handler:\n",
        "#     handler.write(img_data)"
      ],
      "metadata": {
        "id": "L9objhVHnS-h"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Infer using classify/predict.py\n",
        "!python classify/predict.py --weights ./weigths/yolov5s-cls.pt --source bananas.jpg"
      ],
      "metadata": {
        "id": "qqxF5pHCrLd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d70c137-277b-4118-b353-6dffa2d3dcb0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['./weigths/yolov5s-cls.pt'], source=bananas.jpg, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=False, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 üöÄ v7.0-210-gdd10481 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s-cls.pt to weigths/yolov5s-cls.pt...\n",
            "100% 10.5M/10.5M [00:00<00:00, 221MB/s]\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 117 layers, 5447688 parameters, 0 gradients, 11.4 GFLOPs\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/yolov5/classify/predict.py\", line 227, in <module>\n",
            "    main(opt)\n",
            "  File \"/content/yolov5/classify/predict.py\", line 222, in main\n",
            "    run(**vars(opt))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/yolov5/classify/predict.py\", line 104, in run\n",
            "    dataset = LoadImages(source, img_size=imgsz, transforms=classify_transforms(imgsz[0]), vid_stride=vid_stride)\n",
            "  File \"/content/yolov5/utils/dataloaders.py\", line 254, in __init__\n",
            "    raise FileNotFoundError(f'{p} does not exist')\n",
            "FileNotFoundError: /content/yolov5/bananas.jpg does not exist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the output, we can see the ImageNet trained model correctly predicts the class `banana` with `0.95` confidence."
      ],
      "metadata": {
        "id": "yQmj7IXqo3kk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. (Optional) Validate\n",
        "\n",
        "Use the `classify/val.py` script to run validation for the model. This will show us the model's performance on each class.\n",
        "\n",
        "First, we need to download ImageNet."
      ],
      "metadata": {
        "id": "5EosQzyDCk3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # WARNING: takes ~20 minutes\n",
        "# !bash data/scripts/get_imagenet.sh --val"
      ],
      "metadata": {
        "id": "HwAYptjCq-C_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # run the validation script\n",
        "# !python classify/val.py --weights ./weigths/yolov5s-cls.pt --data ../datasets/imagenet"
      ],
      "metadata": {
        "id": "CoHdKXWc8hrD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output shows accuracy metrics for the ImageNet validation dataset including per class accuracy."
      ],
      "metadata": {
        "id": "r2coOcIjuzCO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Train On Custom Data\n",
        "\n",
        "To train on custom data, we need to prepare a dataset with custom labels.\n",
        "\n",
        "To prepare custom data, we'll use [Roboflow](https://roboflow.com). Roboflow enables easy dataset prep with your team, including labeling, formatting into the right export format, deploying, and active learning with a `pip` package.\n",
        "\n",
        "If you need custom data, there are over 66M open source images from the community on [Roboflow Universe](https://universe.roboflow.com).\n",
        "\n",
        "(For more guidance, here's a detailed blog on [training YOLOv5 classification on custom data](https://blog.roboflow.com/train-YOLOv5-classification-custom-data).)\n",
        "\n",
        "\n",
        "Create a free Roboflow account, upload your data, and label.\n",
        "\n",
        "![](https://s4.gifyu.com/images/fruit-labeling.gif)"
      ],
      "metadata": {
        "id": "9bXHHYeVDCXg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Custom Dataset\n",
        "\n",
        "Next, we'll export our dataset into the right directory structure for training YOLOv5 classification to load into this notebook. Select the `Export` button at the top of the version page, `Folder Structure` type, and `show download code`.\n",
        "\n",
        "The ensures all our directories are in the right format:\n",
        "\n",
        "```\n",
        "dataset\n",
        "‚îú‚îÄ‚îÄ train\n",
        "‚îÇ¬†¬† ‚îú‚îÄ‚îÄ class-one\n",
        "‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ IMG_123.jpg\n",
        "‚îÇ¬†¬† ‚îî‚îÄ‚îÄ class-two\n",
        "‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ IMG_456.jpg\n",
        "‚îú‚îÄ‚îÄ valid\n",
        "‚îÇ¬†¬† ‚îú‚îÄ‚îÄ class-one\n",
        "‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ IMG_789.jpg\n",
        "‚îÇ¬†¬† ‚îî‚îÄ‚îÄ class-two\n",
        "‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ IMG_101.jpg\n",
        "‚îú‚îÄ‚îÄ test\n",
        "‚îÇ¬†¬† ‚îú‚îÄ‚îÄ class-one\n",
        "‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ IMG_121.jpg\n",
        "‚îÇ¬†¬† ‚îî‚îÄ‚îÄ class-two\n",
        "‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ IMG_341.jpg\n",
        "```\n",
        "\n",
        "![](https://i.imgur.com/BF9BNR8.gif)\n",
        "\n",
        "\n",
        "Copy and paste that snippet into the cell below."
      ],
      "metadata": {
        "id": "Cu6-lrukD6Hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure we're in the right directory to download our custom dataset\n",
        "import os\n",
        "os.makedirs(\"../datasets/\", exist_ok=True)\n",
        "%cd ../datasets/"
      ],
      "metadata": {
        "id": "6IIgJbP7G6Th",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e945f61-177f-4991-ad1d-151c49741bb2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"eqhaJgc7gBhy0WdJtLEP\")\n",
        "project = rf.workspace(\"my-workspace-jlq0f\").project(\"phan-loai-thoi-tiet-weather-classification\")\n",
        "dataset = project.version(9).download(\"folder\")\n"
      ],
      "metadata": {
        "id": "He6JwHIlG-W_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d981e905-aef9-4a3e-ae58-761ba0e337f1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.1.2-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting certifi==2022.12.7 (from roboflow)\n",
            "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.0.0)\n",
            "Collecting cycler==0.10.0 (from roboflow)\n",
            "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
            "Collecting idna==2.10 (from roboflow)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.23.5)\n",
            "Requirement already satisfied: opencv-python>=4.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.8.0.76)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (9.4.0)\n",
            "Collecting pyparsing==2.4.7 (from roboflow)\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n",
            "Collecting python-dotenv (from roboflow)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n",
            "Collecting supervision (from roboflow)\n",
            "  Downloading supervision-0.13.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.3/59.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.0.4)\n",
            "Collecting wget (from roboflow)\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.0)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.1)\n",
            "Collecting requests-toolbelt (from roboflow)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.1.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.42.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (23.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.2.0)\n",
            "Requirement already satisfied: opencv-python-headless<5.0.0.0,>=4.8.0.74 in /usr/local/lib/python3.10/dist-packages (from supervision->roboflow) (4.8.0.76)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from supervision->roboflow) (1.10.1)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=cc6c3770623f6284402fc87fe870eaac03479b5dbd8c52d72a37bf9e7fe0abcf\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget, python-dotenv, pyparsing, idna, cycler, certifi, supervision, requests-toolbelt, roboflow\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.1.1\n",
            "    Uninstalling pyparsing-3.1.1:\n",
            "      Successfully uninstalled pyparsing-3.1.1\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.4\n",
            "    Uninstalling idna-3.4:\n",
            "      Successfully uninstalled idna-3.4\n",
            "  Attempting uninstall: cycler\n",
            "    Found existing installation: cycler 0.11.0\n",
            "    Uninstalling cycler-0.11.0:\n",
            "      Successfully uninstalled cycler-0.11.0\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2023.7.22\n",
            "    Uninstalling certifi-2023.7.22:\n",
            "      Successfully uninstalled certifi-2023.7.22\n",
            "Successfully installed certifi-2022.12.7 cycler-0.10.0 idna-2.10 pyparsing-2.4.7 python-dotenv-1.0.0 requests-toolbelt-1.0.0 roboflow-1.1.2 supervision-0.13.0 wget-3.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "cycler",
                  "idna",
                  "pyparsing"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Downloading Dataset Version Zip in Ph√¢n-lo·∫°i-th·ªùi-ti·∫øt-(Weather-Classification)-9 to folder: 100% [743870060 / 743870060] bytes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dataset Version Zip to Ph√¢n-lo·∫°i-th·ªùi-ti·∫øt-(Weather-Classification)-9 in folder:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10320/10320 [00:05<00:00, 1957.05it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the dataset name to the environment so we can use it in a system call later\n",
        "dataset_name = dataset.location.split(os.sep)[-1]\n",
        "os.environ[\"DATASET_NAME\"] = dataset_name"
      ],
      "metadata": {
        "id": "wLQbThFICpn4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train On Custom Data üéâ\n",
        "Here, we use the DATASET_NAME environment variable to pass our dataset to the `--data` parameter.\n",
        "\n",
        "Note: we're training for 100 epochs here. We're also starting training from the pretrained weights. Larger datasets will likely benefit from longer training."
      ],
      "metadata": {
        "id": "-5z7Yv42FGrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../yolov5\n",
        "!python classify/train.py --model yolov5s-cls.pt --data $DATASET_NAME --epochs 100 --img 128 --pretrained weights/yolov5s-cls.pt"
      ],
      "metadata": {
        "id": "MXWTTN2BEaqe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30c63cfd-eb40-4b04-ee1a-14b1762b87e6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n",
            "\u001b[34m\u001b[1mclassify/train: \u001b[0mmodel=yolov5s-cls.pt, data=Ph√¢n-lo·∫°i-th·ªùi-ti·∫øt-(Weather-Classification)-9, epochs=100, batch_size=64, imgsz=128, nosave=False, cache=None, device=, workers=8, project=runs/train-cls, name=exp, exist_ok=False, pretrained=weights/yolov5s-cls.pt, optimizer=Adam, lr0=0.001, decay=5e-05, label_smoothing=0.1, cutoff=None, dropout=None, verbose=False, seed=0, local_rank=-1\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ‚úÖ\n",
            "YOLOv5 üöÄ v7.0-210-gdd10481 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train-cls', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mRandomResizedCrop(p=1.0, height=128, width=128, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=1), HorizontalFlip(p=0.5), ColorJitter(p=0.5, brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.6, 1.4], hue=[0, 0]), Normalize(p=1.0, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0), ToTensorV2(always_apply=True, p=1.0, transpose_mask=False)\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s-cls.pt to yolov5s-cls.pt...\n",
            "100% 10.5M/10.5M [00:00<00:00, 121MB/s]\n",
            "\n",
            "Model summary: 149 layers, 4178885 parameters, 4178885 gradients, 10.5 GFLOPs\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001) with parameter groups 32 weight(decay=0.0), 33 weight(decay=5e-05), 33 bias\n",
            "Image sizes 128 train, 128 test\n",
            "Using 1 dataloader workers\n",
            "Logging results to \u001b[1mruns/train-cls/exp\u001b[0m\n",
            "Starting yolov5s-cls.pt training on Ph√¢n-lo·∫°i-th·ªùi-ti·∫øt-(Weather-Classification)-9 dataset with 5 classes for 100 epochs...\n",
            "\n",
            "     Epoch   GPU_mem  train_loss   test_loss    top1_acc    top5_acc\n",
            "     1/100    0.732G        1.02       0.917       0.787           1: 100% 145/145 [01:02<00:00,  2.32it/s]\n",
            "     2/100    0.732G       0.928        1.02       0.667           1: 100% 145/145 [01:00<00:00,  2.39it/s]\n",
            "     3/100    0.732G       0.912        1.02       0.681           1: 100% 145/145 [00:59<00:00,  2.43it/s]\n",
            "     4/100    0.732G       0.886        1.18       0.514           1: 100% 145/145 [01:02<00:00,  2.33it/s]\n",
            "     5/100    0.732G       0.892        1.15       0.569           1: 100% 145/145 [00:59<00:00,  2.42it/s]\n",
            "     6/100    0.732G       0.879        1.37       0.472           1: 100% 145/145 [01:00<00:00,  2.40it/s]\n",
            "     7/100    0.732G       0.857        1.21       0.565           1: 100% 145/145 [01:00<00:00,  2.39it/s]\n",
            "     8/100    0.732G        0.82        1.42       0.509           1: 100% 145/145 [01:01<00:00,  2.37it/s]\n",
            "     9/100    0.732G        0.79        1.17       0.565           1: 100% 145/145 [01:00<00:00,  2.41it/s]\n",
            "    10/100    0.732G       0.786        1.38       0.588           1: 100% 145/145 [01:00<00:00,  2.39it/s]\n",
            "    11/100    0.732G       0.769        1.42       0.569           1: 100% 145/145 [01:00<00:00,  2.40it/s]\n",
            "    12/100    0.732G       0.756        1.42       0.509           1: 100% 145/145 [01:00<00:00,  2.39it/s]\n",
            "    13/100    0.732G       0.734        1.44       0.514           1: 100% 145/145 [01:00<00:00,  2.40it/s]\n",
            "    14/100    0.732G       0.735        1.58       0.551           1: 100% 145/145 [01:01<00:00,  2.37it/s]\n",
            "    15/100    0.732G       0.722         1.4       0.509           1: 100% 145/145 [01:01<00:00,  2.38it/s]\n",
            "    16/100    0.732G       0.713        1.43       0.491           1: 100% 145/145 [01:00<00:00,  2.40it/s]\n",
            "    17/100    0.732G       0.715        1.24       0.583           1: 100% 145/145 [01:00<00:00,  2.40it/s]\n",
            "    18/100    0.732G       0.691        1.23       0.588           1: 100% 145/145 [01:02<00:00,  2.31it/s]\n",
            "    19/100    0.732G        0.69        1.47        0.56           1: 100% 145/145 [01:00<00:00,  2.38it/s]\n",
            "    20/100    0.732G       0.671        1.37        0.56           1: 100% 145/145 [00:59<00:00,  2.45it/s]\n",
            "    21/100    0.732G       0.667        1.34       0.611           1: 100% 145/145 [01:01<00:00,  2.34it/s]\n",
            "    22/100    0.732G       0.654        1.16       0.611           1: 100% 145/145 [01:00<00:00,  2.40it/s]\n",
            "    23/100    0.732G       0.664        1.04        0.69           1: 100% 145/145 [01:01<00:00,  2.34it/s]\n",
            "    24/100    0.732G       0.639        1.11       0.676           1: 100% 145/145 [01:01<00:00,  2.35it/s]\n",
            "    25/100    0.732G        0.63        1.19       0.616           1: 100% 145/145 [01:02<00:00,  2.32it/s]\n",
            "    26/100    0.732G       0.619         1.2       0.579           1: 100% 145/145 [01:00<00:00,  2.41it/s]\n",
            "    27/100    0.732G       0.612         1.3       0.542           1: 100% 145/145 [01:02<00:00,  2.32it/s]\n",
            "    28/100    0.732G       0.617        1.07       0.644           1: 100% 145/145 [01:03<00:00,  2.30it/s]\n",
            "    29/100    0.732G       0.595        1.25       0.597           1: 100% 145/145 [01:00<00:00,  2.41it/s]\n",
            "    30/100    0.732G       0.603        1.18       0.588           1: 100% 145/145 [01:02<00:00,  2.34it/s]\n",
            "    31/100    0.732G       0.588        1.15       0.574           1: 100% 145/145 [01:03<00:00,  2.29it/s]\n",
            "    32/100    0.732G       0.571         1.4       0.523           1: 100% 145/145 [01:06<00:00,  2.19it/s]\n",
            "    33/100    0.732G        0.59        1.18       0.644           1: 100% 145/145 [01:00<00:00,  2.40it/s]\n",
            "    34/100    0.732G       0.577         1.2       0.597           1: 100% 145/145 [01:01<00:00,  2.35it/s]\n",
            "    35/100    0.732G       0.569        1.22       0.602           1: 100% 145/145 [01:02<00:00,  2.32it/s]\n",
            "    36/100    0.732G       0.575        1.17        0.62           1: 100% 145/145 [00:59<00:00,  2.42it/s]\n",
            "    37/100    0.732G       0.576        1.22       0.579           1: 100% 145/145 [01:01<00:00,  2.36it/s]\n",
            "    38/100    0.732G       0.574        1.23       0.583           1: 100% 145/145 [01:02<00:00,  2.34it/s]\n",
            "    39/100    0.732G       0.561        1.31       0.579           1: 100% 145/145 [01:02<00:00,  2.33it/s]\n",
            "    40/100    0.732G       0.555        1.19       0.602           1: 100% 145/145 [01:02<00:00,  2.32it/s]\n",
            "    41/100    0.732G        0.55        1.19        0.62           1: 100% 145/145 [01:02<00:00,  2.33it/s]\n",
            "    42/100    0.732G       0.549        1.25        0.56           1: 100% 145/145 [01:04<00:00,  2.26it/s]\n",
            "    43/100    0.732G       0.541        1.23       0.593           1: 100% 145/145 [01:01<00:00,  2.35it/s]\n",
            "    44/100    0.732G       0.544        1.29        0.56           1: 100% 145/145 [01:02<00:00,  2.32it/s]\n",
            "    45/100    0.732G       0.546        1.32       0.583           1: 100% 145/145 [01:02<00:00,  2.33it/s]\n",
            "    46/100    0.732G       0.532        1.17       0.616           1: 100% 145/145 [01:03<00:00,  2.30it/s]\n",
            "    47/100    0.732G       0.531        1.08       0.662           1: 100% 145/145 [01:00<00:00,  2.40it/s]\n",
            "    48/100    0.732G       0.532        1.24       0.611           1: 100% 145/145 [01:02<00:00,  2.33it/s]\n",
            "    49/100    0.732G       0.542        1.22       0.611           1: 100% 145/145 [01:03<00:00,  2.29it/s]\n",
            "    50/100    0.732G       0.526        1.22       0.616           1: 100% 145/145 [01:01<00:00,  2.36it/s]\n",
            "    51/100    0.732G       0.519        1.22       0.611           1: 100% 145/145 [01:02<00:00,  2.33it/s]\n",
            "    52/100    0.732G       0.526        1.17       0.639           1: 100% 145/145 [01:02<00:00,  2.30it/s]\n",
            "    53/100    0.732G       0.536        1.18       0.634           1: 100% 145/145 [01:04<00:00,  2.27it/s]\n",
            "    54/100    0.732G       0.518        1.12       0.648           1: 100% 145/145 [01:01<00:00,  2.35it/s]\n",
            "    55/100    0.732G       0.514        1.18       0.616           1: 100% 145/145 [01:03<00:00,  2.27it/s]\n",
            "    56/100    0.732G       0.511        1.19       0.606           1: 100% 145/145 [01:04<00:00,  2.24it/s]\n",
            "    57/100    0.732G        0.51        1.22        0.62           1: 100% 145/145 [01:04<00:00,  2.26it/s]\n",
            "    58/100    0.732G       0.507        1.23        0.62           1: 100% 145/145 [01:03<00:00,  2.30it/s]\n",
            "    59/100    0.732G       0.514        1.23       0.606           1: 100% 145/145 [01:01<00:00,  2.36it/s]\n",
            "    60/100    0.732G       0.519         1.2       0.625           1: 100% 145/145 [01:04<00:00,  2.25it/s]\n",
            "    61/100    0.732G       0.507        1.19       0.616           1: 100% 145/145 [01:03<00:00,  2.29it/s]\n",
            "    62/100    0.732G       0.505        1.19       0.625           1: 100% 145/145 [01:03<00:00,  2.29it/s]\n",
            "    63/100    0.732G       0.499        1.17       0.606           1: 100% 145/145 [01:02<00:00,  2.30it/s]\n",
            "    64/100    0.732G       0.502        1.15       0.611           1: 100% 145/145 [01:04<00:00,  2.26it/s]\n",
            "    65/100    0.732G       0.499        1.27       0.597           1: 100% 145/145 [01:05<00:00,  2.22it/s]\n",
            "    66/100    0.732G       0.512        1.17        0.63           1: 100% 145/145 [01:03<00:00,  2.27it/s]\n",
            "    67/100    0.732G       0.496        1.15        0.63           1: 100% 145/145 [01:04<00:00,  2.25it/s]\n",
            "    68/100    0.732G       0.485        1.22       0.611           1: 100% 145/145 [01:00<00:00,  2.38it/s]\n",
            "    69/100    0.732G        0.49        1.19        0.62           1: 100% 145/145 [01:02<00:00,  2.31it/s]\n",
            "    70/100    0.732G       0.491        1.22       0.616           1: 100% 145/145 [01:03<00:00,  2.29it/s]\n",
            "    71/100    0.732G       0.489        1.19        0.62           1: 100% 145/145 [01:02<00:00,  2.32it/s]\n",
            "    72/100    0.732G       0.495         1.2       0.606           1: 100% 145/145 [01:02<00:00,  2.32it/s]\n",
            "    73/100    0.732G       0.484        1.24       0.606           1: 100% 145/145 [01:02<00:00,  2.31it/s]\n",
            "    74/100    0.732G       0.483         1.2        0.62           1: 100% 145/145 [01:03<00:00,  2.28it/s]\n",
            "    75/100    0.732G       0.484        1.19        0.63           1: 100% 145/145 [01:02<00:00,  2.34it/s]\n",
            "    76/100    0.732G       0.478        1.23        0.62           1: 100% 145/145 [01:00<00:00,  2.40it/s]\n",
            "    77/100    0.732G       0.481         1.2       0.625           1: 100% 145/145 [01:03<00:00,  2.27it/s]\n",
            "    78/100    0.732G       0.476        1.21       0.625           1: 100% 145/145 [01:02<00:00,  2.33it/s]\n",
            "    79/100    0.732G       0.483        1.21        0.63           1: 100% 145/145 [01:01<00:00,  2.35it/s]\n",
            "    80/100    0.732G        0.47        1.21        0.62           1: 100% 145/145 [01:01<00:00,  2.38it/s]\n",
            "    81/100    0.732G       0.471        1.19       0.639           1: 100% 145/145 [01:02<00:00,  2.32it/s]\n",
            "    82/100    0.732G       0.471        1.23        0.62           1: 100% 145/145 [01:01<00:00,  2.36it/s]\n",
            "    83/100    0.732G       0.468        1.27        0.62           1: 100% 145/145 [01:00<00:00,  2.41it/s]\n",
            "    84/100    0.732G       0.473        1.24        0.62           1: 100% 145/145 [01:01<00:00,  2.36it/s]\n",
            "    85/100    0.732G       0.464        1.23       0.616           1: 100% 145/145 [01:02<00:00,  2.31it/s]\n",
            "    86/100    0.732G       0.463        1.24        0.63           1: 100% 145/145 [00:59<00:00,  2.42it/s]\n",
            "    87/100    0.732G        0.47        1.21       0.634           1: 100% 145/145 [01:01<00:00,  2.37it/s]\n",
            "    88/100    0.732G       0.468         1.2       0.639           1: 100% 145/145 [01:01<00:00,  2.37it/s]\n",
            "    89/100    0.732G       0.464        1.22       0.639           1: 100% 145/145 [01:01<00:00,  2.37it/s]\n",
            "    90/100    0.732G       0.466        1.22       0.639           1: 100% 145/145 [01:01<00:00,  2.36it/s]\n",
            "    91/100    0.732G       0.465        1.22       0.634           1: 100% 145/145 [01:01<00:00,  2.36it/s]\n",
            "    92/100    0.732G       0.457        1.23       0.644           1: 100% 145/145 [01:00<00:00,  2.39it/s]\n",
            "    93/100    0.732G       0.461        1.19       0.648           1: 100% 145/145 [01:02<00:00,  2.32it/s]\n",
            "    94/100    0.732G       0.456        1.24       0.648           1: 100% 145/145 [01:02<00:00,  2.33it/s]\n",
            "    95/100    0.732G       0.456        1.23       0.639           1: 100% 145/145 [00:59<00:00,  2.42it/s]\n",
            "    96/100    0.732G       0.449        1.24       0.639           1: 100% 145/145 [01:01<00:00,  2.37it/s]\n",
            "    97/100    0.732G        0.45        1.24       0.639           1: 100% 145/145 [01:02<00:00,  2.31it/s]\n",
            "    98/100    0.732G        0.45        1.23       0.634           1: 100% 145/145 [01:00<00:00,  2.41it/s]\n",
            "    99/100    0.732G       0.448        1.26        0.63           1: 100% 145/145 [01:01<00:00,  2.37it/s]\n",
            "   100/100    0.732G       0.445        1.22        0.63           1: 100% 145/145 [01:02<00:00,  2.34it/s]\n",
            "\n",
            "Training complete (1.726 hours)\n",
            "Results saved to \u001b[1mruns/train-cls/exp\u001b[0m\n",
            "Predict:         python classify/predict.py --weights runs/train-cls/exp/weights/best.pt --source im.jpg\n",
            "Validate:        python classify/val.py --weights runs/train-cls/exp/weights/best.pt --data /content/datasets/Ph√¢n-lo·∫°i-th·ªùi-ti·∫øt-(Weather-Classification)-9\n",
            "Export:          python export.py --weights runs/train-cls/exp/weights/best.pt --include onnx\n",
            "PyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'runs/train-cls/exp/weights/best.pt')\n",
            "Visualize:       https://netron.app\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeh9fmi_8Z_l",
        "outputId": "397b9fbb-9677-4c8e-e910-224629572c5d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validate Your Custom Model\n",
        "\n",
        "Repeat step 2 from above to test and validate your custom model."
      ],
      "metadata": {
        "id": "HHUFGeLbGd98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python classify/val.py --weights runs/train-cls/exp/weights/best.pt --data ../datasets/$DATASET_NAME"
      ],
      "metadata": {
        "id": "DIV7ydyKGZFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4654c67e-8faa-4c70-8712-fec2944393c7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mclassify/val: \u001b[0mdata=../datasets/Ph√¢n-lo·∫°i-th·ªùi-ti·∫øt-(Weather-Classification)-9, weights=['runs/train-cls/exp/weights/best.pt'], batch_size=128, imgsz=224, device=, workers=8, verbose=True, project=runs/val-cls, name=exp, exist_ok=False, half=False, dnn=False\n",
            "YOLOv5 üöÄ v7.0-210-gdd10481 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 117 layers, 4173093 parameters, 0 gradients, 10.4 GFLOPs\n",
            "testing: 100% 2/2 [00:02<00:00,  1.00s/it]\n",
            "                   Class      Images    top1_acc    top5_acc\n",
            "                     all         216       0.796           1\n",
            "                  Cloudy         108       0.722           1\n",
            "                    Rain           9           0           1\n",
            "                   Shine          64       0.938           1\n",
            "                 Sunrise          35       0.971           1\n",
            "                 Tornado           0         nan         nan\n",
            "Speed: 0.1ms pre-process, 2.8ms inference, 0.1ms post-process per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/val-cls/exp\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Infer With Your Custom Model"
      ],
      "metadata": {
        "id": "uH5tJNpEsi6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the path of an image from the test or validation set\n",
        "if os.path.exists(os.path.join(dataset.location, \"test\")):\n",
        "  split_path = os.path.join(dataset.location, \"test\")\n",
        "else:\n",
        "  os.path.join(dataset.location, \"valid\")\n",
        "example_class = os.listdir(split_path)[0]\n",
        "example_image_name = os.listdir(os.path.join(split_path, example_class))[0]\n",
        "example_image_path = os.path.join(split_path, example_class, example_image_name)\n",
        "os.environ[\"TEST_IMAGE_PATH\"] = example_image_path\n",
        "\n",
        "print(f\"Inferring on an example of the class '{example_class}'\")\n",
        "\n",
        "#Infer\n",
        "!python classify/predict.py --weights runs/train-cls/exp/weights/best.pt --source 0"
      ],
      "metadata": {
        "id": "81lK1hU_sk54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbdb7cff-1ac1-41c8-9150-6f5935213092"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inferring on an example of the class 'Sunrise'\n",
            "\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp/weights/best.pt'], source=0, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=False, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 üöÄ v7.0-210-gdd10481 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 117 layers, 4173093 parameters, 0 gradients, 10.4 GFLOPs\n",
            "WARNING ‚ö†Ô∏è Environment does not support cv2.imshow() or PIL Image.show()\n",
            "\n",
            "[ WARN:0@3.566] global cap_v4l.cpp:982 open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n",
            "[ERROR:0@3.567] global obsensor_uvc_stream_channel.cpp:156 getStreamChannelGroup Camera index out of range\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/yolov5/classify/predict.py\", line 227, in <module>\n",
            "    main(opt)\n",
            "  File \"/content/yolov5/classify/predict.py\", line 222, in main\n",
            "    run(**vars(opt))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/yolov5/classify/predict.py\", line 99, in run\n",
            "    dataset = LoadStreams(source, img_size=imgsz, transforms=classify_transforms(imgsz[0]), vid_stride=vid_stride)\n",
            "  File \"/content/yolov5/utils/dataloaders.py\", line 367, in __init__\n",
            "    assert cap.isOpened(), f'{st}Failed to open {s}'\n",
            "AssertionError: 1/1: 0... Failed to open 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import cv2\n",
        "import torch\n",
        "\n",
        "# Load YOLOv5 Classification Model\n",
        "model = torch.hub.load('ultralytics/yolov5:v5.0', 'yolov5m')  # Change model size as needed\n",
        "\n",
        "# Initialize webcam\n",
        "cap = cv2.VideoCapture(0)  # 0 for default webcam\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()  # Read a frame from the webcam\n",
        "\n",
        "    # Preprocess frame for classification\n",
        "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
        "    img = model.preprocess(img)  # Preprocess the image\n",
        "\n",
        "    # Perform classification\n",
        "    results = model.predict(img)\n",
        "\n",
        "    # Get predicted class and probability\n",
        "    pred_class = results.pred.argmax().item()\n",
        "    pred_prob = results.pred.max().item()\n",
        "\n",
        "    # Display the result on the frame\n",
        "    label = model.names[pred_class]\n",
        "    cv2.putText(frame, f\"{label}: {pred_prob:.2f}\", (10, 30),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "    # Display the frame with classification result\n",
        "    cv2.imshow('Real-time Classification', frame)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to exit\n",
        "        break\n",
        "\n",
        "# Release the webcam and close the OpenCV windows\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "P7DMyxmhfNoB",
        "outputId": "375f34df-fa9b-4dc4-9ca3-be36c3c2d9c4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/ultralytics/yolov5/zipball/v5.0\" to /root/.cache/torch/hub/v5.0.zip\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-75199f66a324>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load YOLOv5 Classification Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ultralytics/yolov5:v5.0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'yolov5m'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Change model size as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Initialize webcam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m                                            verbose=verbose, skip_validation=skip_validation)\n\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_or_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36m_load_local\u001b[0;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_add_to_sys_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhubconf_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0mhubconf_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhubconf_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODULE_HUBCONF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m         \u001b[0mhub_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_import_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODULE_HUBCONF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhubconf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_entry_from_hubconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhub_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36m_import_module\u001b[0;34m(name, path)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_from_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexec_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_v5.0/hubconf.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myolo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneral\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_requirements\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoogle_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mattempt_download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_v5.0/models/yolo.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautoanchor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_anchor_order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_v5.0/models/common.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mletterbox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneral\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnon_max_suppression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_divisible\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_coords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincrement_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxyxy2xywh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplots\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcolor_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_one_box\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils.datasets'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the inference results show ~3ms inference and the respective classes predicted probabilities."
      ],
      "metadata": {
        "id": "DdGuG-1kNjWT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (OPTIONAL) Improve Our Model with Active Learning\n",
        "\n",
        "Now that we've trained our model once, we will want to continue to improve its performance. Improvement is largely dependent on improving our dataset.\n",
        "\n",
        "We can programmatically upload example failure images back to our custom dataset based on conditions (like seeing an underrpresented class or a low confidence score) using the same `pip` package."
      ],
      "metadata": {
        "id": "I38IM6NXKNN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Upload example image\n",
        "# project.upload(image_path)\n"
      ],
      "metadata": {
        "id": "HycgSEnYKo0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Example upload code\n",
        "# min_conf = float(\"inf\")\n",
        "# for pred in results:\n",
        "#     if pred[\"score\"] < min_conf:\n",
        "#         min_conf = pred[\"score\"]\n",
        "# if min_conf < 0.4:\n",
        "#     project.upload(image_path)"
      ],
      "metadata": {
        "id": "VwXDoz_vLK3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (BONUS) YOLOv5 classify/predict.py Accepts Several Input Methods\n",
        "- Webcam: `python classify/predict.py --weights yolov5s-cls.pt --source 0`\n",
        "- Image `python classify/predict.py --weights yolov5s-cls.pt --source img.jpg`\n",
        "- Video: `python classify/predict.py --weights yolov5s-cls.pt --source vid.mp4`\n",
        "- Directory: `python classify/predict.py --weights yolov5s-cls.pt --source path/`\n",
        "- Glob: `python classify/predict.py --weights yolov5s-cls.pt --source 'path/*.jpg'`\n",
        "- YouTube: `python classify/predict.py --weights yolov5s-cls.pt --source 'https://youtu.be/Zgi9g1ksQHc'`\n",
        "- RTSP, RTMP, HTTP stream: `python classify/predict.py --weights yolov5s-cls.pt --source 'rtsp://example.com/media.mp4'`"
      ],
      "metadata": {
        "id": "aYlfaHDusN-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Directory Example"
      ],
      "metadata": {
        "id": "iKSP-SNTvcLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Directory infer\n",
        "os.environ[\"TEST_CLASS_PATH\"] = test_class_path = os.path.join(*os.environ[\"TEST_IMAGE_PATH\"].split(os.sep)[:-1])\n",
        "print(f\"Infering on all images from the directory {os.environ['TEST_CLASS_PATH']}\")\n",
        "!python classify/predict.py --weights runs/train-cls/exp/weights/best.pt --source /$TEST_CLASS_PATH/"
      ],
      "metadata": {
        "id": "lwSoHcHcvjeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###YouTube Example"
      ],
      "metadata": {
        "id": "kCCao9t8se8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#YouTube infer\n",
        "!python classify/predict.py --weights runs/train-cls/exp/weights/best.pt --source 'https://www.youtube.com/watch?v=7AlYA4ItA74'"
      ],
      "metadata": {
        "id": "heebjpJBsakV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}